{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0c136-c2ac-4f0a-a931-566cf76d7e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification head (binary dialect identification Lithuanian vs Samogitian) \n",
    "# TO DO: if not using the previous fine-tuned model, then change the pathway of the model used in the last code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129104c7-7eb8-4c81-a87b-e3c1e70912cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, load_from_disk, DatasetDict, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import gc\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b5a82-d85c-48fd-8229-9f2b2f401e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model\n",
    "# Load and create test/train for preprocessed datasets\n",
    "\n",
    "samogitian_dataset = load_from_disk(\"processed_corpus/samogitian_dataset\")\n",
    "lithuanian_dataset = load_from_disk(\"processed_corpus/lithuanian_dataset\")\n",
    "\n",
    "# raw splits used by both tokenizers\n",
    "smg_splits = samogitian_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "smg_splits = DatasetDict({\"train\": smg_splits[\"train\"],\"test\": smg_splits[\"test\"]})\n",
    "    \n",
    "lt_splits = lithuanian_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "lt_splits = DatasetDict({\"train\": lt_splits[\"train\"],\"test\": lt_splits[\"test\"]})\n",
    "\n",
    "# label datasets with language ID (used for classification head and used for mlm to balance dataset)\n",
    "smg_train = smg_splits[\"train\"].map(lambda ex: {\"labels\": 0})\n",
    "lt_train  = lt_splits[\"train\"].map(lambda ex: {\"labels\": 1})\n",
    "smg_eval = smg_splits[\"test\"].map(lambda ex: {\"labels\": 0})\n",
    "lt_eval   = lt_splits[\"test\"].map(lambda ex: {\"labels\": 1})\n",
    "\n",
    "print(\"Datatsets with train/test split loaded.\")\n",
    "\n",
    "# Loads Litlat bert\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EMBEDDIA/litlat-bert\") #loads the tokenizer from Hugging Face\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"EMBEDDIA/litlat-bert\")  #loads the model to embed Samogitian in MLM\n",
    "print(f\"Successfully loaded model with {model.num_parameters()} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd2fde-6d4e-471d-a7be-2d085ae8f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset for classification\n",
    "\n",
    "remove_columns=[\"text\", \"source\", \"id\"] \n",
    "columns=[\"input_ids\",\"attention_mask\",\"labels\"] #return the language id label\n",
    "max_length = 256 #less context needed\n",
    "SEED = 42\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def tokenize_ds(dataset, tokenizer):    \n",
    "    def tokenize_function(page):\n",
    "        return tokenizer(\n",
    "            page[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_special_tokens_mask=False,\n",
    "        )\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=64,\n",
    "        num_proc=4,\n",
    "        remove_columns=remove_columns,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    ).with_format(\"torch\", columns=columns)\n",
    "    return tokenized_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "smg_train_cls = tokenize_ds(smg_train, tokenizer)\n",
    "lt_train_cls = tokenize_ds(lt_train, tokenizer)\n",
    "smg_eval_cls = tokenize_ds(smg_eval, tokenizer)\n",
    "lt_eval_cls = tokenize_ds(lt_train, tokenizer)\n",
    "\n",
    "# Downsampling is sufficient for this task. Runs faster with less data, and less is required\n",
    "print(\"Downsampling of Lithuanian dataset:\")\n",
    "lt_train = lt_train.shuffle(seed=SEED).select(range(len(smg_train)))\n",
    "\n",
    "smg_train_cls.save_to_disk(\"tokenized_smg_train_cls\")\n",
    "lt_train_cls.save_to_disk(\"tokenized_lt_train_cls\")\n",
    "smg_eval_cls.save_to_disk(\"tokenized_smg_eval_cls\")\n",
    "lt_eval_cls.save_to_disk(\"tokenized_lt_eval_cls\")\n",
    "train_ds_cls = concatenate_datasets([smg_train_cls, lt_train_cls]).shuffle(seed=SEED)\n",
    "eval_ds_cls  = concatenate_datasets([smg_eval_cls, lt_eval_cls]).shuffle(seed=SEED)\n",
    "train_ds_cls.save_to_disk(\"train_ds_cls\")\n",
    "eval_ds_cls.save_to_disk(\"eval_ds_cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a0d51c-da80-4312-972f-31d58368f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier head for distinguishing Standard Lithuanian and Samogitian using the fine-tuned bert from previous step\n",
    "train_ds_cls = load_from_disk(\"train_ds_cls\")\n",
    "eval_ds_cls = load_from_disk(\"eval_ds_cls\")\n",
    "\n",
    "class LanguageClassifier:\n",
    "    def __init__(self, tokenizer, output_dir=\"./dialect_classifier_model\",\n",
    "                 batch_size=16, learning_rate=3e-5, epochs=1):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def train(self, train_dataset, eval_dataset):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"./samogitian_litlat_bert2\",\n",
    "            num_labels=2,\n",
    "            problem_type=\"single_label_classification\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = model.to(self.device)\n",
    "        model.gradient_checkpointing_enable()\n",
    "        \n",
    "        if self.device.type == 'cuda':\n",
    "            print(f\"GPU Memory after model load: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "        \n",
    "        # metrics and statistics\n",
    "        def compute_metrics(pred):\n",
    "            labels = pred.label_ids\n",
    "            preds = np.argmax(pred.predictions, axis=1)\n",
    "            accuracy = accuracy_score(labels, preds)\n",
    "            f1 = f1_score(labels, preds)\n",
    "            precision = precision_score(labels, preds)\n",
    "            recall = recall_score(labels, preds)\n",
    "            \n",
    "            return {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"f1\": f1,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall\n",
    "            }\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=self.epochs,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size * 2,\n",
    "            gradient_accumulation_steps=2,\n",
    "            learning_rate=self.learning_rate,\n",
    "            warmup_ratio=0.1,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=1000,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=1000,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_accuracy\",\n",
    "            push_to_hub=False,\n",
    "            report_to=\"none\",\n",
    "            fp16=True,\n",
    "            fp16_opt_level=\"O1\",\n",
    "            max_grad_norm=1.0,\n",
    "            dataloader_num_workers=2,\n",
    "            dataloader_pin_memory=True,\n",
    "            logging_steps=100,\n",
    "            logging_first_step=True,\n",
    "            optim=\"adamw_torch\",\n",
    "            gradient_checkpointing=True,\n",
    "            auto_find_batch_size=True\n",
    "        )\n",
    "        \n",
    "        # Training phase\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[\n",
    "                EarlyStoppingCallback(\n",
    "                    early_stopping_patience=3,\n",
    "                    early_stopping_threshold=0.01\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"Starting classifier training...\")\n",
    "        train_result = trainer.train()\n",
    "        print(f\"Training completed in {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "        \n",
    "        print(\"Evaluating classifier...\")\n",
    "        metrics = trainer.evaluate()\n",
    "        print(f\"Classification metrics: {metrics}\")\n",
    "        \n",
    "        trainer.save_model(self.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "        \n",
    "        if self.device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return metrics, model\n",
    "\n",
    "classifier = LanguageClassifier(tokenizer=tokenizer,output_dir=\"./samogitian_litlat_bert_classifier\",\n",
    "    batch_size=16, learning_rate=5e-5,epochs=3)\n",
    "\n",
    "metrics, model = classifier.train(train_dataset=train_ds_cls, eval_dataset=eval_ds_cls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
