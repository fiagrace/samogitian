{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faff8f8-9de1-41cf-9346-943ffc264117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess all data\n",
    "# Data loaded:\n",
    "# - Standard Lithuanin Wikipedia\n",
    "# - Samogitian Wikipedia\n",
    "# - C4 Lithuanian\n",
    "# - Additional Samogitian prose (from CSV files)\n",
    "# - Samogitian crawler (from previous code block)\n",
    "# Uses:\n",
    "# - toxiclt.csv (available from https://huggingface.co/datasets/PeterGraebner/LDNOOBW_V2)\n",
    "\n",
    "# Outputs: processed_corpus contains:\n",
    "#-- processed_samogitian.json     # Samogitian corpus with metadata, json\n",
    "#-- processed_lithuanian.json     # Lithuanian corpus with metadata, json\n",
    "#-- samogitian_corpus.txt         # Samogitian corpus, Plain text version with one doc/entry\n",
    "#-- lithuanian_corpus.txt         # Lithuanian corpus, Plain text version with one doc/entry\n",
    "#-- samogitian_dataset/           # Samogitian corpus, HuggingFace Dataset \n",
    "#-- lithuanian_dataset/           # Lithuanian corpus, HuggingFace Dataset \n",
    "#-- processing_stats.json         # Statistics about preprocessing, json\n",
    "#-- corpus_summary.md             # Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd961d4-9347-4645-9ac6-626ba4e57098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "import unicodedata\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be730f0-d423-4e21-8c95-c7ea6ca931b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"corpus_processing.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"corpus_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c510445-05a9-4408-aaac-35e611efc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all data sources\n",
    "# - Lithuanian Wikipedia\n",
    "# - Samogitian Wikipedia\n",
    "# - Lithuanian C4\n",
    "# - Additional Samogitian prose text (e.g.The Little Prince)\n",
    "\n",
    "def load_all_data_sources() -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    samogitian_data = []\n",
    "    lithuanian_data = []\n",
    "    \n",
    "    logger.info(\"Loading Wikipedia datasets...\")\n",
    "    # Load Samogitian nad Lithuanian Wikipedia\n",
    "    ds_smg = load_dataset(\"wikimedia/wikipedia\", \"20231101.bat-smg\", split=\"train\")   \n",
    "    ds_lt = load_dataset(\"wikimedia/wikipedia\", \"20231101.lt\", split=\"train\") \n",
    "\n",
    "    for item in ds_smg:\n",
    "        samogitian_data.append({\n",
    "            'text': item['text'],\n",
    "            'title': item.get('title', ''),\n",
    "            'source': 'wikipedia_smg',\n",
    "            'url': item.get('url', ''),\n",
    "            'id': item.get('id', '')\n",
    "        })\n",
    "    logger.info(f\"Loaded {len(ds_smg)} Samogitian Wikipedia documents\")\n",
    "        \n",
    "    # Standard Lithuanian Wikipedia\n",
    "    for item in ds_lt:\n",
    "        lithuanian_data.append({\n",
    "            'text': item['text'],\n",
    "            'title': item.get('title', ''),\n",
    "            'source': 'wikipedia_lt',\n",
    "            'url': item.get('url', ''),\n",
    "            'id': item.get('id', '')\n",
    "        })\n",
    "    logger.info(f\"Loaded {len(ds_lt)} Lithuanian Wikipedia documents\")\n",
    "    \n",
    "    # Additional Samogitian prose\n",
    "    logger.info(\"Loading additional Samogitian prose...\")\n",
    "     for fname, src in [(\"moresmg1.csv\", \"little_prince_smg\"), (\"moresmg.csv\", \"prose_smg\")]:\n",
    "        try:\n",
    "            df = pd.read_csv(fname)\n",
    "            for _, row in df.iterrows():\n",
    "                samogitian_data.append({\n",
    "                    \"text\":  row[\"text\"],\n",
    "                    \"source\": src,\n",
    "                    \"id\":     str(row.get(\"id\", \"\")),\n",
    "                    \"label\":  row.get(\"label\", \"\")\n",
    "                })\n",
    "            logger.info(f\"Loaded {len(df)} {src} docs\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {fname}: {e}\")\n",
    "    \n",
    "    # C4 Lithuanian data\n",
    "    logger.info(\"Loading C4 Lithuanian data...\")\n",
    "    ds_c4 = load_dataset(\"allenai/c4\", \"lt\", split=\"train\")\n",
    "    sample_size = min(100000, len(ds_c4))\n",
    "    ds_sample = ds_c4.select(np.random.choice(len(ds_c4), sample_size, replace=False))\n",
    "    mapped_c4 = ds_sample.map(\n",
    "        lambda ex: {\"text\": ex[\"text\"], \"url\": ex.get(\"url\", \"\"), \"source\": \"c4_lt\"},\n",
    "        remove_columns=ds_sample.column_names\n",
    "    )\n",
    "    lithuanian_data.extend(list(mapped_c4))\n",
    "    logger.info(f\"Loaded {sample_size} C4 Lithuanian docs\")\n",
    "    \n",
    "    # Files from Samogitian crawler\n",
    "    logger.info(\"Loading targeted crawler data...\")\n",
    "    crawler_docs = []\n",
    "    try:\n",
    "        json_files = []\n",
    "        if os.path.exists(\"samogitian_corpus\"):\n",
    "            json_files.extend(glob(\"samogitian_corpus/*.json\"))\n",
    "            for sub in os.listdir(\"samogitian_corpus\"):\n",
    "                subp = os.path.join(\"samogitian_corpus\", sub)\n",
    "                if os.path.isdir(subp):\n",
    "                    json_files.extend(glob(f\"{subp}/*.json\"))\n",
    "        json_files = [f for f in json_files if not f.endswith((\"stats.json\", \"summary.json\", \"corpus.json\"))]\n",
    "\n",
    "        for jf in json_files:\n",
    "            with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc = json.load(f)\n",
    "            if isinstance(doc, dict) and \"text\" in doc:\n",
    "                crawler_docs.append(doc)\n",
    "            elif isinstance(doc, list):\n",
    "                crawler_docs.extend([d for d in doc if isinstance(d, dict) and \"text\" in d])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading targeted crawler data: {e}\")\n",
    "\n",
    "    for doc in crawler_docs:\n",
    "        samogitian_data.append({\n",
    "            \"text\":   doc[\"text\"],\n",
    "            \"title\":  doc.get(\"title\", \"\"),\n",
    "            \"url\":    doc.get(\"url\", \"\"),\n",
    "            \"source\": doc.get(\"source\", \"targeted_crawl\"),\n",
    "            \"domain\": doc.get(\"domain\", \"\")\n",
    "        })\n",
    "    logger.info(f\"Loaded {len(crawler_docs)} crawler docs\")\n",
    "\n",
    "    logger.info(f\"Total: {len(samogitian_data)} Samogitian, {len(lithuanian_data)} Lithuanian\")\n",
    "    return samogitian_data, lithuanian_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855f811-f6a9-4924-8a03-d48f30b4947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Checks for quality:\n",
    "# - Quality: reasonable text length, not excessively repetitive or filled with special characters\n",
    "# - Removes toxic words\n",
    "# - Ensures only relevant languages (Lithuanian and Samogitian dialect)\n",
    "# - Ensures document not already repeated\n",
    "# - Boilerplate removal\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for corpus processing\"\"\"\n",
    "    min_text_length: int = 60 \n",
    "    max_text_length: int = 100000 \n",
    "    lang_detection_threshold: float = 0.6 # confidence of language identification \n",
    "    repetition_thresh: float = 0.4 \n",
    "    special_char_thresh: float = 0.6 \n",
    "    toxic_csv_path: str = \"toxiclt.csv\" #words from https://huggingface.co/datasets/PeterGraebner/LDNOOBW_V2\n",
    "\n",
    "class CorpusProcessor:    \n",
    "    def __init__(self, output_dir: str = \"processed_data\", config: Config = Config()):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.config = config\n",
    "\n",
    "        #Load Fasttext language identification model (glotlid best for Samogitian)\n",
    "        model_path = hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\", cache_dir=None)\n",
    "        self.lang_model = fasttext.load_model(model_path)\n",
    "\n",
    "        # Load toxic words\n",
    "        nsfw_df = pd.read_csv(self.config.toxic_csv_path)\n",
    "        self.nsfw_terms = {\n",
    "            term\n",
    "            for col in nsfw_df.columns\n",
    "            for term in nsfw_df[col].dropna().str.lower()\n",
    "        }\n",
    "    \n",
    "    def identify_language(self, text: str) -> Tuple[str, float]:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        labels, probabilities = self.lang_model.predict(text, k=1) #only keep top k=1 predicted language\n",
    "        lang = labels[0].replace('__label__', '').split('_')[0]\n",
    "        return lang, probabilities[0]\n",
    "    \n",
    "    def quality_check(self, text: str) -> Tuple[float, str]:\n",
    "        if len(text) < self.config.min_text_length:\n",
    "            return 0.0, \"too_short\"\n",
    "        if len(text) > self.config.max_text_length:\n",
    "            return 0.0, \"too_long\"\n",
    "\n",
    "        #check for excessive repetitions using Jaccard similarity nad word diversity\n",
    "        text_block_size = 100\n",
    "        if len(text) < text_block_size * 2:\n",
    "            return 0.0, \"short\"\n",
    "        text_blocks = [text[i:i+text_block_size] for i in range(0, len(text) - text_block_size, text_block_size)]\n",
    "        repetitions = 0\n",
    "        for i in range(len(text_blocks) - 1):\n",
    "            for j in range(i + 1, min(i + 5, len(text_blocks))):\n",
    "                set1 = set(text_blocks[i])\n",
    "                set2 = set(text_blocks[j])\n",
    "                intersection = len(set1.intersection(set2))\n",
    "                union = len(set1.union(set2))\n",
    "                jaccard_sim = intersection / max(1, union)\n",
    "                if jaccard_sim > 0.8:\n",
    "                    repetitions += 1\n",
    "        rep_ratio = repetitions/max(1, len(text_blocks) - 1)\n",
    "        if rep_ratio > self.config.repetition_thresh:\n",
    "            return 0.0, \"repetitive_content\"\n",
    "        words = text.split() \n",
    "        word_diversity = len(set(words))/max(1,len(words))\n",
    "        \n",
    "        # Check for excessive special characters\n",
    "        special_char_ratio = sum(1 for c in text if not c.isalnum() and not c.isspace()) / max(1, len(text))\n",
    "        if special_char_ratio > self.config.special_char_thresh:\n",
    "            return 0.0, \"excessive_special_chars\"\n",
    "\n",
    "        # remove documents with toxic words\n",
    "        text_lower = text.lower()\n",
    "        for term in nsfw_terms:\n",
    "            if term in text_lower:\n",
    "                return 0.0, \"toxic\"\n",
    "        \n",
    "        quality_score = min(1.0, word_diversity * 2)\n",
    "        return quality_score, \"acceptable\"\n",
    "        \n",
    "    def remove_boilerplate(self, text: str) -> str:\n",
    "        indicators = [\n",
    "            r'Copyright © \\d{4}.*?(\\n|$)',\n",
    "            r'All rights reserved.*?(\\n|$)',\n",
    "            r'Visos teisės saugomos.*?(\\n|$)',\n",
    "            \n",
    "            r'Privacy Policy.*?(\\n|$)',\n",
    "            r'Terms of Service.*?(\\n|$)',\n",
    "            r'Privatumo politika.*?(\\n|$)',\n",
    "            r'Naudojimo sąlygos.*?(\\n|$)',\n",
    "            r'Slapukų politika.*?(\\n|$)',\n",
    "            r'Cookies.*?(\\n|$)',\n",
    "            r'Slapukai.*?(\\n|$)',\n",
    "            \n",
    "            # Citations and references\n",
    "            r'\\[\\d+\\]', \n",
    "            r'References\\s*:.*?(\\n\\n|$)',\n",
    "            r'Nuorodos\\s*:.*?(\\n\\n|$)',\n",
    "            r'Nūruodas\\s*:.*?(\\n\\n|$)',\n",
    "            r'Šaltiniai\\s*:.*?(\\n\\n|$)',\n",
    "            r'Bibliography\\s*:.*?(\\n\\n|$)',\n",
    "            r'Literatūra\\s*:.*?(\\n\\n|$)',\n",
    "            r'Šaltenē\\s*:.*?(\\n\\n|$)'\n",
    "        ]\n",
    "        \n",
    "        for indicator in indicators:\n",
    "            text = re.sub(indicator, ' ', text, flags=re.IGNORECASE)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    def compute_text_hash(self, text):\n",
    "        t = unicodedata.normalize(\"NFKC\", text).lower()\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "        return hashlib.md5(t.encode()).hexdigest()\n",
    "    \n",
    "    def _process_language_corpus(self, corpus: List[Dict[str, Any]], language_name: str) -> List[Dict[str, Any]]:\n",
    "        filtered_corpus = []\n",
    "        text_hashes = set()\n",
    "        expected_lang = \"sgs\" if language_name == \"samogitian\" else \"lit\"\n",
    "        \n",
    "        for doc in tqdm(corpus, desc=f\"Filtering {language_name}\"):\n",
    "            clean_text = self.remove_boilerplate(doc['text'])\n",
    "            quality_score, reason = self.quality_check(clean_text)\n",
    "            if quality_score < 0.5:\n",
    "                continue\n",
    "            # Skip duplicates\n",
    "            text_hash = self.compute_text_hash(clean_text)\n",
    "            if text_hash in text_hashes:\n",
    "                continue\n",
    "            text_hashes.add(text_hash)\n",
    "\n",
    "            lang, confidence = self.identify_language(clean_text)\n",
    "            if lang != expected_lang and confidence > self.config.lang_detection_threshold:\n",
    "                continue\n",
    "\n",
    "            processed_doc = {\n",
    "                'text': clean_text,\n",
    "                'language_confidence': confidence,\n",
    "                'text_hash': text_hash\n",
    "            }\n",
    "            for key in ['title', 'url', 'source', 'id']:\n",
    "                if key in doc:\n",
    "                    processed_doc[key] = doc[key]\n",
    "            \n",
    "            filtered_corpus.append(processed_doc)\n",
    "        \n",
    "        output_file = self.output_dir / f\"processed_{language_name}.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(filtered_corpus, f, ensure_ascii=False, indent=2)\n",
    "        text_file = self.output_dir / f\"{language_name}_corpus.txt\"\n",
    "        with open(text_file, 'w', encoding='utf-8') as f:\n",
    "            for doc in filtered_corpus:\n",
    "                f.write(doc['text'] + \"\\n\\n\")\n",
    "        \n",
    "        dataset = Dataset.from_dict({\n",
    "            \"text\": [doc[\"text\"] for doc in filtered_corpus],\n",
    "            \"source\": [doc.get(\"source\", \"unknown\") for doc in filtered_corpus],\n",
    "            \"id\": [doc.get(\"id\", str(i)) for i, doc in enumerate(filtered_corpus)]\n",
    "        })\n",
    "        dataset.save_to_disk(str(self.output_dir / f\"{language_name}_dataset\"))\n",
    "        \n",
    "        return filtered_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfff6b-9545-412f-b69f-04fbf5890f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    samogitian_data, lithuanian_data = load_all_data_sources()\n",
    "    processor = CorpusProcessor(output_dir=\"processed_corpus\")\n",
    "    processed_samogitian = processor._process_language_corpus(samogitian_data, \"samogitian\")\n",
    "    processed_lithuanian = processor._process_language_corpus(lithuanian_data, \"lithuanian\")\n",
    "    \n",
    "    processor.logger.info(f\"Processing complete. saved to: {\"processed_corpus\"}\")\n",
    "    processor.logger.info(f\"Samogitian corpus: {len(processed_samogitian):,} documents\")\n",
    "    processor.logger.info(f\"Lithuanian corpus: {len(processed_lithuanian):,} documents\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
