{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2efa3-54a7-41f8-8a87-edcd4cce5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data: Crawl for Samogitian Language\n",
    "# Results stored in samogitian_corpus.json and samogitian_corpus.txt\n",
    "# Stats available: crawler_stats.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ef4c9-b727-4451-8d87-fe9918644e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 0: Load packages\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import requests\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209fd5a-ceb9-4468-b056-e1b4f67f2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targeted common crawler for Samogitian Language\n",
    "    \n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"samogitian_crawler.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"samogitian_crawler\")\n",
    "\n",
    "class SamogitianWebCrawler:    \n",
    "    def __init__(self, output_dir=\"samogitian_corpus\"):\n",
    "        self.output_dir = output_dir\n",
    "        self.language_threshold = 0.6\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # statistics for logging\n",
    "        self.stats = defaultdict(lambda: {\n",
    "            \"pages_visited\": 0,\n",
    "            \"samogitian_pages\": 0,\n",
    "            \"errors\": defaultdict(int)\n",
    "        })\n",
    "        \n",
    "        # Load fasttext model (glotlid works best with Samogitian)\n",
    "        logger.info(\"Loaded lang id model.\")\n",
    "        model_path = hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\", cache_dir=None)\n",
    "        self.lang_model = fasttext.load_model(model_path)\n",
    "        \n",
    "    def is_samogitian(self, text, threshold=None):\n",
    "        if not text or len(text.strip()) < 20:\n",
    "        return False\n",
    "        text = text.replace('\\n', ' ').strip()\n",
    "        labels, probabilities = self.lang_model.predict(text[:1000], k=2)\n",
    "        return labels[0] == '__label__sgs_Latn' and probabilities[0] > threshold\n",
    "    \n",
    "    def extract_text(self, html):\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'meta', 'noscript']):\n",
    "                tag.decompose()\n",
    "            \n",
    "            # Get text with paragraph breaks\n",
    "            paragraphs = []\n",
    "            for p in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'article', 'section', 'div']):\n",
    "                text = p.get_text(strip=True)\n",
    "                if text and len(text) > 10: \n",
    "                    paragraphs.append(text)\n",
    "            return \"\\n\\n\".join(paragraphs)\n",
    "    \n",
    "    def crawl_website(self, start_url, max_pages=500, max_depth=5):\n",
    "        logger.info(f\"Crawling {start_url}\")\n",
    "        # Parse domain\n",
    "        parsed_url = urlparse(start_url)\n",
    "        base_domain = parsed_url.netloc\n",
    "        \n",
    "        # Initialize crawl (url, depth)\n",
    "        visited = set()\n",
    "        to_visit = [(start_url, 0)]\n",
    "        samogitian_pages = []\n",
    "        \n",
    "        # Create domain directory\n",
    "        domain_dir = os.path.join(self.output_dir, base_domain.replace('.', '_'))\n",
    "        os.makedirs(domain_dir, exist_ok=True)\n",
    "        \n",
    "        while to_visit and len(visited) < max_pages:\n",
    "            url, depth = to_visit.pop(0)\n",
    "            #skip repeats\n",
    "            if url in visited:\n",
    "                continue\n",
    "            self.stats[base_domain][\"pages_visited\"] += 1\n",
    "            \n",
    "            if depth > max_depth:\n",
    "                continue\n",
    "            \n",
    "            visited.add(url)\n",
    "            logger.info(f\"Visiting: {url} (depth {depth})\")\n",
    "            \n",
    "            try:\n",
    "                # fortimeouts\n",
    "                response = requests.get(\n",
    "                    url, \n",
    "                    timeout=30,\n",
    "                    verify=False,  \n",
    "                    headers={\n",
    "                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Check if successful\n",
    "                if response.status_code != 200:\n",
    "                    logger.warning(f\"Got status code {response.status_code} for {url}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract text\n",
    "                text = self.extract_text(response.text)\n",
    "                if self.is_samogitian(text):\n",
    "                    logger.info(f\"Found Samogitian content: {url}\")\n",
    "                    self.stats[base_domain][\"samogitian_pages\"] += 1\n",
    "                    \n",
    "                    page_number = self.stats[base_domain][\"samogitian_pages\"]\n",
    "                    filename = f\"{domain_dir}/{page_number}.json\"\n",
    "                    title = \"\"\n",
    "                    try:\n",
    "                        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                        if soup.title:\n",
    "                            title = soup.title.get_text(strip=True)\n",
    "                    except Exception:\n",
    "                        pass                \n",
    "                    page_data = {\n",
    "                        \"url\": url,\n",
    "                        \"text\": text,\n",
    "                        \"title\": title,\n",
    "                        \"domain\": base_domain,\n",
    "                        \"source\": \"targeted_crawl\"\n",
    "                    }\n",
    "                    \n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(page_data, f, ensure_ascii=False, indent=2)\n",
    "                    samogitian_pages.append(page_data)\n",
    "\n",
    "                # extract links with same domain\n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                for a in soup.find_all(\"a\", href=True):\n",
    "                    href = a[\"href\"]\n",
    "                    if href.startswith(\"#\") or href.startswith(\"javascript:\"):\n",
    "                        continue\n",
    "                    full = urljoin(url, href)\n",
    "                    p = urlparse(full)\n",
    "                    if p.netloc == base_domain:\n",
    "                        norm = f\"{p.scheme}://{p.netloc}{p.path}\"\n",
    "                        if p.query:\n",
    "                            norm += \"?\" + p.query\n",
    "                        if norm not in visited:\n",
    "                            to_visit.append((norm, depth + 1))\n",
    "                            \n",
    "                for link in links:\n",
    "                    if link not in visited:\n",
    "                        to_visit.append((link, depth + 1)) #adds new link\n",
    "            \n",
    "            except requests.exceptions.SSLError as e:\n",
    "                logger.error(f\"SSL Error processing {url}: {e}\")\n",
    "                self.stats[base_domain][\"errors\"][\"ssl\"] += 1\n",
    "            \n",
    "            except requests.exceptions.Timeout as e:\n",
    "                logger.error(f\"Timeout processing {url}: {e}\")\n",
    "                self.stats[base_domain][\"errors\"][\"timeout\"] += 1\n",
    "            \n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                logger.error(f\"Connection error processing {url}: {e}\")\n",
    "                self.stats[base_domain][\"errors\"][\"connection\"] += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {url}: {e}\")\n",
    "                self.stats[base_domain][\"errors\"][\"other\"] += 1\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        self._save_stats()\n",
    "\n",
    "        with open(os.path.join(self.output_dir, \"crawler_stats.json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(dict(self.stats), f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"Found {len(samogitian_pages)} Samogitian pages on {start_url}\")\n",
    "        return samogitian_pages\n",
    "    \n",
    "    def crawl_multiple(self, websites):\n",
    "        all_pages = []\n",
    "        for website in websites:\n",
    "            url = site if site.startswith(\"http\") else f\"http://{site}\"\n",
    "            try:\n",
    "                all_pages.extend(self.crawl_website(website))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error crawling {website}: {e}\")\n",
    "        \n",
    "        combined_json = self.output_dir / \"samogitian_corpus.json\"\n",
    "        with combined_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_pages, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        combined_txt = self.output_dir / \"samogitian_corpus.txt\"\n",
    "        with combined_txt.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for pg in all_pages:\n",
    "                f.write(pg[\"text\"].replace(\"\\n\", \" \") + \"\\n\")\n",
    "        \n",
    "        return all_pages\n",
    "\n",
    "def main():\n",
    "    import urllib3\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "   \n",
    "    crawler = SamogitianWebCrawler(output_dir=\"samogitian_corpus\")\n",
    "    websites = [\n",
    "        \"https://zkd.lt\",\n",
    "        \"https://zemaitiuzeme.lt\",\n",
    "        \"https://zemaiciukalba.lt\",\n",
    "        \"https://skouds.lt\"\n",
    "    ]\n",
    "    all_pages = crawler.crawl_multiple(websites)\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"\\nCrawling complete!\")\n",
    "    print(f\"Found {len(all_pages)} Samogitian pages across {len(websites)} websites\")\n",
    "    \n",
    "    # Print stats\n",
    "    print(\"\\nPages by domain:\")\n",
    "    for domain, stats in crawler.stats.items():\n",
    "        s, v = stats[\"samogitian_pages\"], stats[\"pages_visited\"]\n",
    "        print(f\"  {domain}: {s}/{v} pages ({s/max(1,v)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nErrors by type:\")\n",
    "    all_errors = defaultdict(int)\n",
    "    for domain_stats in crawler.stats.values():\n",
    "        for error_type, count in domain_stats[\"errors\"].items():\n",
    "            all_errors[error_type] += count\n",
    "    for error_type, count in all_errors.items():\n",
    "        print(f\"  {error_type}: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
